---
title: 'NeurIPS 2024 - 云端目标检测大模型自适应首篇！'
date: 2024-12-10
modified: 2024-12-10
collection: posts
permalink: /blogs/coin/
header:
  teaser: "COIN/thumbnail.png"
inner:
  teaser: "COIN/title.png"
tags:
  - cloud object detector adaptation
  - knowledge integration
  - teacher-student
  - CLIP
  - gradient
  - gradient alignment

keyword: '**关键词:** 云端目标检测大模型自适应、知识传播、知识分离、知识蒸馏、梯度方向对齐'
excerpt: '基于云端目标检测大模型自适应的API，迁移其到本地检测器'
explain: '**转载自:** [原文地址](https://zhuanlan.zhihu.com/p/11666611412)'

paperurl: 'https://openreview.net/pdf?id=S8SEjerTTg'
codeurl: 'https://github.com/Flashkong/COIN'
projecturl: '/projects/coin'
videourl: ''
slidesurl: '/files/COIN/COIN_slides.pdf'
posterurl: '/files/COIN/COIN_poster.pdf'

blogurl: '/blogs/coin'
zhihuurl: 'https://zhuanlan.zhihu.com/p/11666611412'
xiaohognshuurl: ''

---


<h1 style="margin: 0.5em 0 0.5em;" >摘要</h1><hr/>
<div style="text-align: justify;">{{"
我们提出了一个有趣且符合时代潮流的问题——云端目标检测大模型自适应（Cloud Object Detector Adaptation, **CODA**），其中目标域利用由云端目标检测大模型提供的检测结果训练目标域检测器。尽管云端大模型具有强大的泛化能力，但在特定目标域中仍然无法实现无错误的检测。因此，我们提出了一种新的云端目标检测大模型自适应方法，即通过整合不同源知识的**COIN**（**C**loud **O**bject detector adaptation method by **I**ntegrating different source k**N**owledge），其核心思想是引入一个公开的视觉-语言模型（CLIP），通过自促进梯度方向对齐来蒸馏正知识并优化负知识以实现自适应。为此，**知识传播、分离和蒸馏**被经依次进行。**1.知识传播**首先将云端检测器和CLIP模型的知识结合起来，用于在目标域中初始化目标域检测器和CLIP检测器。**2.知识分离**使用分而治之的策略将CLIP检测器和云端检测器的检测结果相匹配，并将它们分为一致的、不一致的和私有的三部分，以便逐个进行知识蒸馏。**3.知识蒸馏**逐个击破，其中一致和私有的检测结果直接用于训练目标域检测器，而不一致的检测结果则首先被一个提出的一致性知识生成网络进行深度融合，然后再被蒸馏入目标域检测器。由于一致检测的梯度方向提供了指向最优目标域检测器的方向，因此我们使用它作为监督信号，将不一致检测的梯度方向与它对齐来训练该网络。实验结果表明，所提出的COIN方法达到了最先进的性能水平。

云端目标检测大模型自适应（CODA）与其他相近问题设定之间的区别，以及所提出的方法（COIN）的整体概括图1所示：
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/idea1.png" | prepend: "/images/" | prepend: base_path }}" alt="图1 论文整体概括图">
  <figcaption>图1 论文整体概括图</figcaption>
</div>

<h1 style="margin: 0.5em 0 0.5em;" >云端目标检测大模型自适应的详细定义</h1><hr/>

<div style="text-align: justify;  margin-bottom: 1.3em;">
云端目标检测大模型自适应（CODA）假设无标签的目标域为$\mathcal{D}=\{ x^i\}_{i = 1}^{N_t}$，且$\mathcal{C}=\{c^i\}_{i = 1}^{N_c}$是一个需要被检测的类别集合，$N_t$表示目标域类别数量，$c^i$表示第 个类别名，$N_c$表示类别数量。在云端，存在一个强大的目标检测大模型$F_{\theta _{cld}}$，它通过API输出请求图片$x^i$的检测结果$ y_{cld}^i$。CODA的目标是基于$x^i$和$ y_{cld}^i$训练一个目标域检测器。CODA借助在大规模标注数据集上训练的强大的云检测器，使得开放的目标域场景甚至是类别的自适应成为可能。
</div>

<h1 style="margin: 0.5em 0 0.5em;" >算法细节</h1><hr/>

<div class="myteaser_old">
  <img src="{{ "COIN/overview.png" | prepend: "/images/" | prepend: base_path }}" alt="图2 算法细节图">
  <figcaption>图2 算法细节图</figcaption>
</div>
<div style="text-align: justify;">{{"
所提出的COIN方法引入了视觉-语言模型CLIP[1]来辅助自适应一个可以任意选择的云端目标检测大模型（如GDINO[2]）。如图2所示，该方法包含三个阶段：知识传播、知识分离和知识蒸馏。

1. 在知识传播阶段，首先分别收集云端检测器和CLIP的检测结果。以GDINO为例，它的检测框和CLIP的分类结果被用来预训练CLIP检测器和初始化目标域检测器，如图1(b)所示。如图2(a)所示，预训练时，对 CLIP 检测器同时进行提示学习（prompt learning），使CLIP的知识进一步传播并自适应到目标域。

2. 在知识分离阶段，如图2(b)所示，首先获取来自云端检测器和 CLIP 检测器的检测结果，其次通过匹配将他们分为一致、不一致和私有三类。

3. 在知识蒸馏阶段，我们采用了Mean-Teacher框架，其中将云端检测器和CLIP检测器视为两个教师模型，而目标域检测器视为学生模型。为了增强模型的鲁棒性，学生模型接收经过强增强处理的目标域图像输入，一致和私有检测结果则直接作为伪标签使用。对于不一致的检测结果，我们设计了一个一致性知识生成网络（Consistent Knowledge Generation network, CKG）来在决策层面进行融合；同时提出了一种自促进梯度方向对齐损失来优化 CKG，使得目标域检测器和CKG相互更新。融合的知识通过指数移动平均（Exponential Moving Average，EMA）机制不断更新CLIP检测器，从而实现更好的知识融合。

" | markdownify}}</div>

<h2 style="margin: 0.5em 0 0.5em;" >第一阶段：知识传播</h2>

<div style="text-align: justify; margin-bottom: 1.3em;">
如图2(a)所示，CLIP检测器$F_{\theta _{clip}}$和目标域检测器$F_{\theta _{T}}$采用相同的检测架构——基于Faster R-CNN。其中CLIP的视觉编码器被用作特征提取器；一个变换网络被加入以捕捉定位能力；CLIP的文本编码器被用作分类头；可训练的提示被加入以自适应CLIP到目标域。
</div>

<div style="text-align: justify; ">{{"
为了便于后续的分离和蒸馏，我们首先通过知识传播预训练一个CLIP检测器，该步骤借助了云端检测器和CLIP模型的知识——同时利用云端模型的检测框和CLIP对这些框的分类结果作为标签来训练。具体的损失函数如下：
" | markdownify}}</div>

<div style="text-align: justify;">
$$\displaystyle{\min_{\theta _{clip}}} \; \mathcal{L}_{RPN} + \mathcal{L}_{ROI} + \lambda \mathcal{L}^1_{align}$$
</div>

<div style="text-align: justify; margin-bottom: 1.3em;">
其中$\mathcal{L}^1_{align}$是提示学习（prompt learning）的损失函数，用来更新可学习的提示（prompts）。$e$代表语义嵌入，而$e_p$代表语义嵌入的原型，它由视觉特征逐步更新得到。
</div>
<div style="text-align: justify;">
$${ e_p^i} = \eta \cdot { e_p^i} + (1-\eta) \cdot \mathbb{E}_{x \in \mathcal{D}}
\frac{1}{|\mathcal{R}|} \sum \mathbb{1}( l=i)  f$$
$$\mathcal{L}^1_{align} = || e_p - \,  e||_1$$
</div>


<h2 style="margin: 0.5em 0 0.5em;" >第二阶段：知识分离</h2>

<div style="text-align: justify; ">{{"
就像同时抛掷两枚**硬币（COIN）**一样，云端检测器和CLIP检测器的检测结果由于各自预训练数据的不同，既可能存在一致性，也可能出现冲突。显然，一致的检测结果可以作为可靠的真实值使用，而冲突则会为知识融合带来挑战。为了合理地将这两者的知识整合到目标域检测器中，我们采用了“**分而治之**”的策略，本小节为“分”，而下一小节则为“合”，具体如下：
" | markdownify}}</div>
<div style="text-align: justify; margin-bottom: 1.3em;">
如图2(b)所示，对于任意一张目标域图片$x$，云端检测器的检测结果$ y_{cld}=\{ b_{cld},  p_{cld}\}$与CLIP检测器的检测结果$ y_{clip}=\{ b_{clip}, p_{clip}\}$通过IoU进行匹配，得到辨别矩阵$\Gamma$；如果云端检测器的第$i$个框和CLIP检测器的第$j$个框的IoU大于0.5，则$\Gamma_{i,j}=1$，否则$\Gamma_{i,j}=0$。基于该判别矩阵，我们构建一致性检测集合$\hat{\mathcal{P}}$，不一致性检测集合$\tilde{\mathcal{P}}$以及私有检测集合$\mathcal{Q}$如下：
</div>
<div style="text-align: justify;">
$$\hat{\mathcal{P}} = \{( y_{cld}^i,  y_{clip}^j)  \, | \, \Gamma_{i,j}=1, {l}_{cld}^{i} = {l}_{clip}^{j}\}, 
\tilde{\mathcal{P}} = \{( y_{cld}^i,  y_{clip}^j)  \, | \, \Gamma_{i,j}=1, {l}_{cld}^{i} \neq {l}_{clip}^{j}\}$$
$$\mathcal{Q} = \{ y_{cld}^i \, | \, \Gamma_{i,*}=0\} \cup \{ y_{clip}^j \, | \, \Gamma_{*,j}=0\}$$
</div>
<div style="text-align: justify; ">{{"
从公式中可以看出，一致性检测代表匹配且预测类别相同的检测结果；不一致性检测代表匹配但预测类别不同的检测结果；私有检测代表没有匹配的检测结果。对于一致性检测和不一致性检测中的匹配框，我们基于概率加权将他们融合：
" | markdownify}}</div>
<div style="text-align: justify;">
$$ b_{m}^t = \frac{\displaystyle\max_{c} {p}_{cld,c}^{i} *  b_{cld}^i + \displaystyle\max_{c} {p}_{clip,c}^{j} *  b_{clip}^j}{\displaystyle\max_{c} {p}_{cld,c}^{i} + \displaystyle\max_{c} {p}_{clip,c}^{j}}$$
</div>
<div style="text-align: justify; margin-bottom: 1.3em;">
最终，一致性检测集合可以被表示为$\hat{\mathcal{P}}=\{(\hat{ b}_{m}\,, \hat{ p}_{cld}\,, \hat{ p}_{clip}\,, \hat{ l}_{m})\}$，而不一致性检测可以被表示为$\tilde{\mathcal{P}}=\{(\tilde{ b}_{m}\,, \tilde{ p}_{cld}\,, \tilde{ p}_{clip}\,, \tilde{ l}_{cld}\,, \tilde{ l}_{clip})\}$。
</div>

<h2 style="margin: 0.5em 0 0.5em;" >第三阶段：知识蒸馏</h2>

<div class="myteaser_old">
  <img src="{{ "COIN/ckg.png" | prepend: "/images/" | prepend: base_path }}" alt="图3 一致性知识生成网络结构图">
  <figcaption>图3 一致性知识生成网络结构图</figcaption>
</div>

<div style="text-align: justify; ">{{"
该阶段分别将以上三种检测结果合理地融合到目标域检测器中；Mean-Teacher框架和EMA参数更新机制被使用，将融合知识向CLIP检测器流动，从而实现更好的知识融合。

对于一致性检测和私有检测，一致性检测被用当作真实标签用于监督训练，私有检测被当作候选框输入网络中，并仅通过蒸馏分类概率实现知识融合：
" | markdownify}}</div>

<div style="text-align: justify;">
$$\mathcal{L}_{con} = \mathcal{L}_{RPN} + \mathcal{L}_{ROI}$$
$$\mathcal{L}_{pri} = L_{kl}( p_q^{stu},  p_q)$$
</div>

<div style="text-align: justify; margin-bottom: 1.3em;">
对于不一致性检测，我们设计了一个一致性知识生成网络（Consistent Knowledge Generation network, CKG）来在决策层面进行融合，它的网络结构如图3所示。首先，不一致性检测被分别用以构建两个视觉特征类别原型$\tilde{ e}_{p}^{cld}$和$\tilde{ e}_{p}^{clip}$，其次学生模型输出的不一致性特征$\tilde{ f}_{stu}$通过交叉注意力模块分别与两个类别原型计算对应的权重$ w_{cld}$与$ w_{clip}$，最终权重被叠加到云端检测器和CLIP检测器的原始概率上，并通过softmax函数输出修正结果。具体过程如下面的公式所示：
</div>

<div style="text-align: justify;">
$$ w_{cld} = CA_1(\tilde{ f}_{stu},\tilde{ e}_{p}^{cld}),  w_{clip} = CA_2(\tilde{ f}_{stu}, \tilde{ e}_{p}^{clip})$$
$$\tilde{ p}_{ckg} = \delta( w_{cld} \odot \tilde{ p}_{cld} +  w_{clip} \odot \tilde{ p}_{clip})$$
</div>

<div style="text-align: justify; margin-bottom: 1.3em;">
虽然CKG网络被构建，但如何训练它依旧是一个难点。考虑到一致性检测可以被当作目标域的真实标签，它们提供的优化方向因此朝着最优检测器的优化方向，所以该优化方向可以作为一个监督信号来指导CKG网络训练。因此我们提出了一种自促进梯度方向对齐损失，通过约束CKG网络输出的预测结果$\tilde{ p}_{ckg}$对学生模型产生的梯度来更新CKG网络。一致性检测和不一致性检测产生的梯度如下：
</div>

<div style="text-align: justify;">
$$\hat{ g} = \nabla_{\theta_{T}} \|\hat{ p}_{stu}- \, \mathbb{I}(\hat{ l}_m)\|_2, \quad
\tilde{ g} = \nabla_{\theta_{T}} \|\tilde{ p}_{stu}- \, \tilde{ p}_{ckg}\|_2 $$
</div>

<div style="text-align: justify; ">{{"
除了梯度对齐外，CKG网络还要保证能够在一致性检测上输出正确的预测结果，因此训练CKG网络的总损失如下：
" | markdownify}}</div>

<div style="text-align: justify;">
$$\displaystyle{\min_{\theta _{ckg}}} \; \mathcal{L}_{ckg} = (1 - sim(\hat{ g}, \tilde{ g})) + L_{kl}(\hat{ p}_{ckg}, \, \mathbb{I}(\hat{ l}_m))$$
</div>

<div style="text-align: justify; ">{{"
CKG网络与目标域检测器迭代更新，首先CKG网络基于目标域检测器的梯度优化更新，随后目标域检测器又通过CKG网络输出的预测结果优化更新：
" | markdownify}}</div>

<div style="text-align: justify;">
$$\mathcal{L}_{inc} = L_{kl}(\tilde{ p}_{stu}^{\pi}, \, \tilde{ p}_{ckg}^{\pi})$$
$$\displaystyle{\min_{\theta _{T}}} \; \mathcal{L}_{con} + \gamma_1 \mathcal{L}_{inc} +\gamma_2 \mathcal{L}_{pri} + \lambda \mathcal{L}^2_{align}$$
</div>

<div style="text-align: justify; ">{{"
算法流程如如图4所示：
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/algorithm.png" | prepend: "/images/" | prepend: base_path }}" alt="图4 算法流程图">
  <figcaption>图4 算法流程图</figcaption>
</div>

<h1 style="margin: 0.5em 0 0.5em;" >实验</h1><hr />

<h2 style="margin: 0.5em 0 0.5em;" >基准数据集的性能</h2>

<div style="text-align: justify; ">{{"
我们报告了常用的六个数据集：Foggy-Cityscapes、BDD100K、KITTI、Cityscapes、Sim10K以及Clipart的实验结果。由于这是首篇关于CODA的工作，因此我们将COIN与UDAOD、SFOD以及Black-box DAOD的方法对比，且使用GDINO作为云端目标检测大模型，具体结果如图5-7所示，其中CLIP代表使用云端检测器的框以及CLIP模型的分类结果；CLIP检测器代表经过知识传播阶段预训练后输出的CLIP检测器。
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/results1.png" | prepend: "/images/" | prepend: base_path }}" alt="图5 实验结果">
  <figcaption>图5 实验结果</figcaption>
</div>

<div class="myteaser_old">
  <img src="{{ "COIN/results2.png" | prepend: "/images/" | prepend: base_path }}" alt="图6 实验结果">
  <figcaption>图6 实验结果</figcaption>
</div>

<div class="myteaser_old">
  <img src="{{ "COIN/results3.png" | prepend: "/images/" | prepend: base_path }}" alt="图7 实验结果">
  <figcaption>图7 实验结果</figcaption>
</div>

<h2 style="margin: 0.5em 0 0.5em;" >消融实验</h2>

<div style="text-align: justify; ">{{"
我们针对每一个损失函数以及不同的训练阶段进行了消融试验，具体结果如图8所示。可以看到，每个部件都为整体结果发挥着积极的作用。
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/ablation.png" | prepend: "/images/" | prepend: base_path }}" alt="图8 消融实验">
  <figcaption>图8 消融实验</figcaption>
</div>

<h2 style="margin: 0.5em 0 0.5em;" >不同决策级融合策略性能对比</h2>

<div style="text-align: justify; ">{{"
为了进一步验证我们决策级融合的有效性，我们将提出的COIN与其他四种策略进行了对比，结果如图9所示，单独使用CLIP检测器达到了38.1%的mAP，这是因为它的EMA更新机制使它融入了来自于云端检测器的知识。而使用CKG网络则取得了最优的效果，这证明了我们提出的决策级融合的有效性。
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/decision.png" | prepend: "/images/" | prepend: base_path }}" alt="图9 决策级融合策略性能对比图">
  <figcaption>图9 决策级融合策略性能对比图</figcaption>
</div>

<h2 style="margin: 0.5em 0 0.5em;" >模型检测速度对比</h2>

<div style="text-align: justify; ">{{"
我们对比了在测试阶段和实际部署阶段目标域检测器与云端检测器的检测速度，如图10所示。在实际部署时，目标域检测器的文本编码器可以被丢掉，直接使用语义嵌入计算分类概率以增加计算速度。从图中可以看到，相比于云端检测器，即使在不考虑API请求时延的情况下，我们的检测器依旧完胜。
" | markdownify}}</div>

<div class="myteaser_old">
  <img src="{{ "COIN/speed.png" | prepend: "/images/" | prepend: base_path }}" alt="图10 检测速度对比">
  <figcaption>图10 检测速度对比</figcaption>
</div>

<h1 style="margin: 0.5em 0 0.5em;" >补充</h1><hr />
<div style="text-align: justify;">{{"
**请注意，对于是否使用CLIP，CODA并不做限制，即使在我们的方法COIN使用了CLIP。**
" | markdownify}}</div>


<h1 style="margin: 0.5em 0 0.5em;" >总结</h1><hr />
<div style="text-align: justify; ">{{"
1. 我们提出了一个适用于当前潮流、适用于云端大模型的、具有广阔应用前景的问题——云端目标检测大模型自适应（CODA）。为此，我们提出了一种新颖的COIN方法，并采用了“分而治之”的策略来解决该问题。通过引入一个开放的辅助模型（CLIP）来帮助模型自适应，并且通过巧妙地整合不同源知识，实现了“1+1>2”的效果。

2. 提出了一种新颖的决策级融合策略。通过设计梯度方向对齐损失，利用一致性检测结果，以一种合理且自促进的方式来融合冲突检测结果。

3. 针对CLIP检测器和目标域检测器进行了不同的提示学习。对于CLIP检测器，通过将语义嵌入与CLIP的视觉特征类别原型对齐来进行训练；而对于目标域检测器，由于其结合了不同源知识，因此使用基于一致性检测结果的类别原型进行学习。

" | markdownify}}</div>

<h1 style="margin: 0.5em 0 0.5em;" >引用</h1><hr />
[1] Learning transferable visual models from natural language supervision.

[2] Grounding dino: Marrying dino with grounded pre-training for open-set object detection.

<hr/>

感谢阅读！如果觉得本文有帮助，请为我们的[开源代码](https://github.com/Flashkong/COIN)点个⭐️吧~

